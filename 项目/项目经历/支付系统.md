# 支付系统



## 使用场景

充值、转账、提现、退款、发红包、抢红包、扫码收款。





## 系统的演进过程

讲述最开始设计是怎样的，然后出现了什么问题，然后怎么修复。

- 指标：我们期望系统能承受每秒上万的QPS
- 最开始的支付系统，为了快速迭代快发，所有的**功能都耦合在一个系统中**（支付系统1.0）
- 随着用户量的增大，越来越多的问题暴露出来：
  - 所有的功能都耦合在一起，只要某一个模块出了问题，很可能整个系统都不可用了
  - 单节点上，导致系统整体性能不高
  - 这时候**只能做到纵向扩展来提高系统的能力，比如说使用8核 16g内存的机器，SSD固态硬盘，引入缓存。**
- 服务化拆分：（支付系统2.0）
  - 划分模块：支付业务、支付平台、资金系统、订单系统、支付网关、对账系统
  - 每个模块独立部署4台机器，形成集群
  - 效果：
    - 服务独立部署，提供了系统容灾能力，并且提高了性能。
    - **系统支持增加机器来提高性能，拥有了横向扩展的能力。**
    - 整个系统的模块都是集群，提供了故障转移的能力。
    - 使用dubbo作为RPC框架，提供了超时控制、降级、限流。
- 系统整体优化（支付系统3.0）
  - 随着访问量增大，出现数据一致性问题，比如说支付了金钱，但是没有生成红包，这就导致一致性问题。从而导致资金错乱。
    - 增加补偿机制，自动修复脏数据
    - 数据库操作增加重试，还有告警
    - 增加调账机制，用户反馈了资金有问题，并且我核实了存在问题，那么可以对用户的钱包进行调账
  - 所有业务都用到订单系统，订单数据库压力增大，因此采用分表，分为16张表，从而提高订单表的写性能
  - 所有中间件，如MySQL、Redis都使用集群，避免单点故障
  - 关键模块添加限流保护，比如说支付平台就使用了阻塞队列进行限流（桶令牌算法）
  - 进行压测，抢红包的QPS为12000/s，达到指标要求。
    - 抢红包只是一个Redis的内存操作，就直接返回结果了。而Redis的QPS可以达到接近4万左右，所以抢红包12000的QPS是很正常的。





<br/>





## 支付系统架构

![image-20201003225322117](https://tva1.sinaimg.cn/large/007S8ZIlgy1gjcjaq9luvj31800oudn9.jpg)





## 支付系统流程演示

![image-20191021120137035](https://tva1.sinaimg.cn/large/006y8mN6gy1g85otau9pqj30kp0cx409.jpg)

### 支付平台

- 支付平台是支付系统的核心模块，有提现、转账、充值、退款这些基础功能
- 负责和支付业务进行对接



### 支付业务

重点业务有：红包、扫码收款。

- 红包
  - 分为普通红包、拼手气红包。
  - 普通红包是整个红包，比如说红包总金额是5块钱，发出的红包只有1个5块的红包，只能发送给单个人员
  - 拼手气红包会把整个红包划分为很多个小红包，比如说红包金额是5块，总共2个红包，那么这5块钱会被分成2个小红包，每个小红包的金额是随机的。拼手气红包只可以发到群里面。
- 扫码收款
  - 扫码收款分为订单收款、普通收款
  - 收款这个业务场景是：收款方通过服务商模式，把微信及支付宝账号绑定到我们公司的收款账号，收款方展示二维码，付款方拿出微信或者支付宝进行扫码，然后进行支付。收款方的微信或者支付宝就收到钱了。
  - 订单收款和普通收款的区别在于，订单收款会在收款以后，将收款信息封装成一个订单数据，然后同步到CRM的销售订单对象中，然后客户可以通过CRM的销售订单对象看到这个收款信息。



### 订单系统

- 负责订单的维护，比如生成订单，更新订单
- 所有的支付业务公用一套订单系统



###  资金系统

- 负责管理用户的资金账户
- 可以进行资金的调整



### 支付网关

- 用于支付系统和第三方交互的模块
- 把验签、请求第三方的代码抽离出来，便于维护



### 对账系统

- 对账就是把自己系统的订单跟第三方（比如说支付宝、微信）的订单进行金额的对比。
- 每天凌晨对前一天的交易进行对账，发现异常则告警，从而提高系统的可靠性。



### 风控系统

使用Drools规则引擎，制定规则实现风控。



### 验证系统

- 概述：为了防止抓包重发，比如有黑客使用暴力破解的方式，生成了一个有效的订单号，然后用这个订单号进行资金操作，从而造成资金错乱。因此在进行资金操作之前，客户端需要先获取一个token，然后支付系统对这个token进行校验，判定token有效，再进行资金操作。
- 流程：
  - 生成token及序列号（存在redis中，1个小时过期），异步推送给客户端。客户端用携带token和新的序列号请求后台，后台校验token及序列号有效，则进行正常的业务流程
- 作用：防止抓包重放。  
- 因为token是异步推送给客户端的，所有无法用脚本请求接口获取。
- 怎么生成token及序列号？

  - token类型 + 企业账号 +  员工id + 时间戳 + 4位自增数字
- 序列号：初始值为5位数的随机值，每次客户端加1然后再发起请求
- 怎么验证？
  - token没有过期（token还在redis中）
  - key是token，field是序列号，value是一个固定常量。如果该序列号已经使用过（hsetnx失败），则验证失败，否则把这个没有使用过的序列号存储下来。
  - token可以在一段时间内有效，序列号单次有效。使用序列号是为了加强安全性——即使token被人获取了，黑客无法知道当前有效的序列号是多少，也很难通过校验
- redis中的数据
  - key-value映射      token:初始化序列号
  - token: {序列号："1"} 









## 主要功能

### 支付业务

#### 红包

- 发红包：
  - 生成红包业务数据（比如说红包业务id），返回红包id
  - 调用充值接口，生成订单、流水，返回**预支付订单号**
  - 客户端通过**预支付订单号**，调起第三方支付，比如说微信支付
  - 用户支付完成，微信侧回调商户后台
  - 支付平台更新订单、流水状态。并且把这笔钱添加到该用户的冻结金额中
  - 支付平台回调红包业务系统
    - 首先生成n个小红包，（拼手气红包）红包金额随机，并且入库
      - 生成红包的金额的算法如下：
        - 计算剩余的最大金额、剩余的最小金额（剩余的最大金额的计算：如果还要计算n个小红包金额，那么n-1都是发最小的红包金额，当前的红包就是最大的金额了。 剩余的最小金额同理）
        - 使用随机数生成一个金额，然后判断是否满足最大金额、最小金额的限制
        - 如果大于最大金额，则赋值为最大金额。（小于最小金额同理）
        - 把剩余的总金额 减去 当前计算出的小红包金额
    - 把这n个小红包的金额放到redis队列中，使用list数据结构实现队列
    - 把大红包（整个红包的数据）存入redis中，用于提高读性能
    - 调用推送消息给终端，让其展示红包
- 抢红包：
  - 写请求并发比较大：
    - 首先从redis队列中出队一个金额
    - 利用hsetnx指令实现分布式锁，向redis中存入小红包的信息（抢红包的人的userId、抢了红包的金额）
    - 通过MQ异步执行耗时操作，比如说更新红包数据到数据库、执行转账操作
      - 调用支付平台转账功能（发红包者的冻结金额减少，抢红包者的可用金额增加）
      - 这里MQ起到缓冲的作用，并且保证高可用。
      - 如何保证写请求不会对数据库产生太大压力，同时保证性能，保证数据一致性？
        - 方法：写缓存、通过mq异步更新数据到数据库
        - 由于写缓存成功，并且发送mq消息以后，立即返回响应给用户，所以性能非常高，一般RT在10毫秒左右
        - 通过mq起到削峰填谷的作用，并且保证了缓存和数据库的一致性。只有更新数据库成功，才返回消费成功，否则返回消费失败，这样子mq就会重复发送mq消息，从而保证更新数据库成功
        - 消费端做了幂等：首先请求分布式锁锁住这个大红包，然后判断这个人是否抢了这个大红包下面的任意一个小红包，如果没有，则根据该小红包的金额，查询出一个小红包，如果该小红包没有被抢，则更新为已被这个人抢了，执行转账操作。
  - 读请求并发量比较大：
    - 使用缓存来提高读性能。
- 红包详情
  - 先查询redis，在查询数据库
- 订单补偿
  - 每隔10分钟，扫描最近30天的红包列表，如果过期，则退款
  - 每隔10分钟，扫描最近30天的**已抢红包列表**，抢到红包的话，正常情况下转账订单号不为空。如果转账订单为空，说明支付平台当时可能发生错误了，没有进行转账操作，那么再次触发转账。





####扫码收款

- 扫码收款（以微信为例子）
  - 获取二维码url
  - 客户端生成二维码
  - 用户扫码二维码，跳转到商户h5页面
  - 输入金额，点击付款
  - 支付平台创建订单、流水
  - 支付平台调用微信接口，生成预支付订单号prepayId
  - 有了prepayId，客户端就可以通过jsapi调起第三方支付（比如说微信）
  - 用户支付成功
  - 第三方系统（比如说微信）回调支付平台
  - 支付平台修改订单、流水状态
  - 支付平台回调业务后台
  - 业务后台对业务数据进行处理（比如说 扫码收款有一条业务数据，设置为已完成状态）



### 支付平台

#### 提现

##### 快钱

概述：使用快钱的**付款到个人银行账户**

流程：用户绑定银行卡——》提现申请——》生成提现订单、流水——》后台审批——》发送提现申请到第三方——》获取提现结果——》修改订单、流水状态



##### 微信

概述：使用企业付款的方式

流程：用户发起提现——》申请生成提现订单、流水——》冻结资金——》后台审批——》发送提现申请到第三方——》获取提现结果——》修改订单、流水状态

参考：https://pay.weixin.qq.com/wiki/doc/api/tools/mch_pay.php?chapter=14_2



#### 充值

##### 支付宝

流程：用户发起充值——》生成订单、流水——》拼装URL（支付宝提供给商户的服务接入网关URL：https://mapi.alipay.com/gateway.do? + 业务参数）——》跳转到支付宝——》用户扫码付款——》回调商户后台——》操作资金系统（加钱）——》修改订单、流水状态



##### 微信

流程：用户发起充值——》生成订单、流水——》调用微信接口，获取二维码连接——》生成二维码——》用户扫码支付——》微信回调商户后台——》操作资金系统（加钱）——》修改订单、流水状态



#### 转账

概述：目前只是抢红包使用到转账功能

流程（假设A转账给B）：用户A通过充值发出一个红包——》用户A被冻结了一笔钱X——》用户B抢红包——》用户B生成一个入账订单——》操作资金（分为两步，A的冻结金额减去X，X解冻为已用；B的可用金额增加X）——》操作资金成功的话，修改订单、流水状态。否则，回滚



#### 退款

概述：红包过期退款、提现审批不通过导致退款。

红包过期退款**流程**：红包过期了，要把剩余金额退还用户——》创建退款订单、流水——》操作资金系统退款（目前toc使用的是微信退款API）——》退款成功，修改订单、流水状态

提现审批不通过导致退款**流程**：审批不通过——》把用户提现的时候冻结的金额解冻——》修改提现订单、流水为**无效订单**的状态。



#### 回调

支付平台处理完订单、流水、资金之后，会回调业务方，这里的回调使用**HTTP回调 + 失败重试**的方式。

具体做法为当前服务既是某个topic的发送者，也是该topic的消费者。

如果回调失败，就把请求内容封装成一个消息扔到MQ队列中，MQ隔一段时间会重新推送这个消息，当前应用接收到该消息，重新发起HTTP回调，直到回调成功，从而保证消息可达。







## 涉及技术



### 分库分表

通过实现主从架构，提高并发读性能，因为从库也分担了一部分读请求。同时保障高可用，因为主库挂了，还能够进行主从切换，把某个从库提升为主库。

但是，QPS上来之后，读写请求都增加了：

- 单个节点查询会越来越慢。因为数据量越来越多，索引占用的空间越来越大，那么数据库无法把索引都加载到内存中，只能从磁盘查询索引页，这样影响了查询性能。
- 系统如何处理更高的并发写入请求？

对付这些问题，就可以使用分库分表技术。分库分表技术有垂直拆分、水平拆分。



#### 垂直拆分

咱们的支付系统也采用了垂直分库，所谓的垂直分库，就是按照业务来划分表，比如说用户表和订单表就分离开来，放在不同的数据库中，并且这些数据库单独放置在不同的物理机器上，让不同业务表充分使用其机器IO和内存等资源。如果垂直分库还不满足性能需求了，这时候就用上水平拆分。

- 通过把不同业务的数据拆分到不同数据库节点上，这样某个节点发生故障时，只影响该节点的功能。当然，每个节点都通过主从架构来保障高可用，所以这种情况基本不会发生。
- 对数据库进行垂直拆分是比较常使用的。



#### 水平拆分

通过垂直拆分，将不同业务的表分离到不同数据库节点上，但是单个表如果数据量很大，还是扛不住大量的读写请求，这时候就可以使用水平拆分技术。

咱们公司的水平拆分使用**sharding-jdbc框架**来实现。具体为，订单系统使用sharding-jdbc进行分片，根据订单号来进行分表，分为16张表。订单号作为全局唯一ID。

**查询单个数据**：查询数据的时候，先通过订单号进行hash取模，得到数据放置在哪一张表上，然后去该表上进行查询。

**h5分页查询**：通过时间戳倒序分页查询数据，在sharding-jdbc中会实现为把请求转发到每张表上，通过where条件筛选出满足的数据列表，比如说每张表都筛选出20条数据，这些数据都小于lastId，然后做一次合并操作，那么15张表，就有20*15=300条数据，这300条数据在内存中根据创建时间排序，并且进行倒序，然后取出前面20条件，就是咱们需要的数据。

**分页查询**：需要先在不同的分片节点中将数据进行排序并返回，然后将不同分片返回的结果集进行汇总和再次排序，最终返回给用户。

![image-20191123112742668](https://tva1.sinaimg.cn/large/006y8mN6gy1g97ta87otrj30zm0oy7gs.jpg)

上图中只是取第一页的数据，对性能影响还不是很大。但是如果取得页数很大，情况则变得复杂很多，因为各分片节点中的数据可能是随机的，为了排序的准确性，**需要将所有节点的前N页数据都排序好做合并**，最后再进行整体的排序，在取出偏位位置的那一页，这样的操作时很**耗费CPU和内存资源的**，所以页数越大，系统的性能也会越差。

参考：[分库分表](https://cloud.tencent.com/developer/article/1464281)







#### 全局唯一性ID

**咱们系统目前的方案**

咱们的订单系统使用订单号来作为分区键，订单号是一个32位的字符串是如下格式：

2位的业务前缀 + 1位的订单号版本 +  4位预留 + 4位用户账号 + 17位时间戳（毫秒级） + 4位自增号

- 业务前缀表示该订单是哪一类业务。使得订单号具有业务相关性，这样子找问题也比较简单。
- 订单号版本表示这个订单是哪个版本，比如我现在使用第一版的订单号格式，后面可能还会出第二版的订单号格式。
- 自增号到9999之后，会重新变为1开始。



**其他方案：通过雪花算法改进**

除了我公司目前的做法，我觉得还可以采用雪花算法。

业务前缀 + 机器IP + 时间戳 + 自增序列号





### redis缓存

通过将热数据（也就是还没有过期的红包）放到redis中，当抢红包的时候，一大波写请求过来，QPS异常高，通过 redis缓存 + 消息队列 + 分库分表 + 限流 抗住压力。

当抢红包的请求过来时，用户抢红包数据先更新到redis中，成功更新则说明抢到红包，通过hsetnx实现分布式锁，保证抢红包的数据正确性。

更新完redis的数据后，发送MQ消息，通知另外一个业务服务去进行转账操作、并且更新红包数据库信息。这里MQ起到了一个缓冲的作用，因为更新数据库的请求首先会在MQ消息队列中进行排队。当数据库中红包数据也被标志为已抢，那么数据不一致的问题也解决了。

由于咱们发红包，抢红包的热数据都在redis中，接来下用户访问这些热数据通过redis来访问，能大大提高读写性能，减少数据库压力。

**redis使用sential集群！！！**





#### 红包补偿机制

红包过期了，怎么返还金额；抢红包完成后，需要通过支付平台将发红包账户的钱转移到抢红包账户中，那么假如这个过程出故障了，怎么保障系统自动修复，继续运行？

- 定时扫码过期的红包，如果已经过期了，就把冻结的剩余金额退款到用户账户上
- 定期扫码转账订单号为空的红包，这些红包是抢红包之后转账失败的，那么就是重新发起转账流程







### 分布式锁

抢红包那里使用hsetnx实现分布式锁，但是在集群中是可能有问题的：
比如在sentinel集群，假如主节点setnx成功了，然后没有同步给从节点，然后主节点挂了，那从节点提升为主节点之后，又可以再次setnx成功了，因此存在加了两把锁的情况，可能导致1个人抢到2个红包。

这时候可以使用RedLock来解决，但是RedLock实际上还不完善，并且有它自己的缺点，如果实在要考虑RedLock，还不如直接使用Zookeeper作为分布式锁。



### 乐观锁

使用乐观锁保证数据安全，比如说资金表会有一个版本号字段，每次更新操作都会更新版本号，如果有并发更新操作，那么后面的操作会失败，然后重试。





### 流量控制

**基于阻塞队列queue的offer实现**

原理，初始化队列，默认长度（可同时并发执行的数量），每一个请求进来时，往队列放置一个对象，如果队列已满，则等待，然后超时。请求执行完后，释放队列位置。

使用方式：

1. 配置流控aop 

2. 在需要流控限制的接口上配置注解 





### 支付安全

- 存储安全**：
  - 将敏感数据采用AES加密后存储；读取的时候，采用AES解密。
- **通讯安全**：
  - 使用HTTPS加密
  - 传递的参数需要进行验签
    - 通过验签来保证参数没有被改变，那假如sign签名本身就被替换了呢？  
      - 首先签名是通过**密钥+参数**生成的，就算签名被替换了，但是黑客无法获取密钥，因此黑客无法使用密钥生成正确的签名，接收方会验签失败。
- **资金安全**：
  - 风控：使用Drools规则引擎，制定规则实现风控。
    - 每天提现次数不能超过3次
    - 每天提现金额不能超过1万
  - 对账：
    - 外部对账：每天凌晨2点，下载第三方的前一天的账单，和内部账单进行金额对比。
    - 内部对账：校验每家企业，计算规则为   充值+收入 -提现 -消费 = 余额
- **身份认证**：
  - 商户身份验证：登录账户的密码校验
  - 快钱银行卡身份认证
  - 手机验证码验证
- **Token验证**
  - 目的是：避免抓包重放
  - 开始执行业务请求之前，客户端向Validator服务获取token
  - Validator服务异步推送token。之所以采用异步推送的形式，是为了防止脚本恶意获取Token。因为只有咱们的客户端才能使用异步推送。
  - 客户端携带token到后台进行校验（token是否在有效期、是否被使用过），校验通过，执行业务逻辑。





## 价值

- 从功能上
  - **提供便捷、可靠的支付能力**，为用户创造价值
    - 如基础支付能力，有充值、转账、提现、退款
    - 再如，扫码收款方便企业之间收款，并且提供收款统计，为企业提供价值。扫码收款月流水达到1400万
    - 再如，红包帮助同事间更好建立关系，管理层也可以通过红包来激励员工
- 从技术上
  - 为用户提供可靠的支付服务
    - 补偿机制、限流、对账
  - **支持快速接入新的支付业务，良好的扩展性**，提高开发效率
    - 前端组件、业务维护业务订单号
- 跟竞品比较
  - toB这套支付系统提供了企业级支付能力，比如说企业之间进行扫码收款，统计收款数据，接入CRM订单对象。 这些能力我暂时没有在其他支付系统里面看到。







## 发展方向

支付系统发展方向(业务上的思考）：

红包应该和营销联系在一起，比如说推广某篇文章，把这篇文章转发到某个群里面，那么用户阅读完文章之后，可以领到1个红包。从而激励用户阅读文章，提高营销推广的效果。





## 问题

**支付系统的QPS及如何抗住高并发？**

咱们抢红包的监控数据QPS是800/s。  压测的QPS为12000/s。   

主要做了如下措施：

- Nginx做前端请求接入层的负载均衡
  - 采用轮询的负载均衡策略
- **支付平台接入层限流**
  - 阻塞队列实现限流，队列长度为200（每个接入层web服务抗住200 qps/s）
  - 接入层有4台机器
- **服务拆分**：
  - 单台机器的参数是8核16G内存，采用多路复用的IO模型。
  - 使用Dubbo作为RPC框架，提供了服务治理、消费方负载均衡等功能。
  - 将支付系统按功能拆分为多个服务，每个服务配置4台机器作为集群
  - 实际上nginx、web层、service层、数据库、缓存每一层都是机器集群，只不过机器数量可能不一致
  - 服务拆分以后，不同服务部署在不同机器上面，独立的模块集群化，提高了容灾能力。
- **数据库拆分**
  - 红包的数据库做了主从分离，提高了读的性能，写数据库是通过消息队列异步写的，所以不会对数据库有太大的压力。
  - 垂直拆分
    - 按业务拆分，不同的业务表拆分在不同的数据库上，不同数据库放到不同机器上，使得该业务表可以独占该机器资源。
    - 不同的业务库单独使用主从分离
    - 不同的服务使用不同的数据库连接池，方便解耦。 而且不同服务使用不同的数据库，进一步提高了性能。
  - 水平拆分
    - 按分区键来水平划分数据
    - 订单表按订单号拆分成16张表（如果有需要，每个分表库可以单独再做主从分离来保证高可用）
  - 数据库使用连接池，最小连接数10，最大30。（为啥设置那么小？ 因为MySQL的IO模型是使用多路复用IO，一条线程就可以处理成千上万的请求）
    - 我们使用的数据库连接池是C3P0
- **Redis集群（sentinal）**
  - 单次Redis的写请求RT为10毫秒。 Redis的写请求QPS可达10万每秒。
  - 保证缓存高可用
- **RocketMQ异步更新**，起到削峰填谷的作用
- 使用**SSD固态硬盘**，作为数据库存储，寻址快，提高数据库性能从而提高系统性能。
- **监控系统告警**
- **补偿机制**
  - 过期红包退款
  - 抢红包失败的订单，重新发起转账
- **日志全备份**
  - 保证出现问题的时候能够还原现场

支付系统支撑高并发的能力的技术有：**读写缓存、多路复用IO模型、异步更新数据库、红包库使用数据库主从分离、订单库使用分库分表**

<br/>



**支付平台回调过程中有哪些参数？**

商品号（唯一标识一个业务）、订单号、金额、请求时间、订单状态



<br/>



**一个集群有好几个redis节点，同时收到hsetnx请求，那么会不会发生这样的情况：几个redis节点同时判断key没有，然后就插入。**

**问题的原意是：redis能作为分布式锁的根本原因是什么：原子性**

- 不会。**同一个key会发送到同一个redis节点中**，**并且redis命令具有原子性**，setnx指令是包括查询、写入的2个操作，这两个操作是原子执行的，因此可以保证分布式锁的实现。
- 另外一种情况，比如在 Sentinel 集群中，主节点挂掉时，从节点会取而代之，客户端上却并没有明显感知。原先第一个客户端在主节点中申请成功了一把锁，但是这把锁还没有来得及同步到从节点，主节点突然挂掉了。然后从节点变成了主节点，这个新的节点内部没有这个锁，所以当另一个客户端过来请求加锁时，立即就批准了。这样就会导致系统中同样一把锁被两个客户端同时持有，不安全性由此产生。这种情况下，可以使用RedLock红锁来解决。 https://juejin.im/book/5afc2e5f6fb9a07a9b362527/section/5b4c19216fb9a04fb8773ed1



<br/>



**红包里面，1个人同时点了很多下抢红包，产生2个请求，一个请求来到服务A，一个请求来到服务B，那么怎么保证这个人不会抢到2个红包？（如何保证一个人不会抢到多个红包）**

利用hsetnx有3个参数，分别是 key，field，value。  这个key是红包id，field是员工id，value是抢红包详情，如果员工id已经存在，则hsetnx失败，通过这一点，保证了不会重复抢红包



<br/>



**多个人同时抢，如何保证不会抢到同一个红包？**

咱们红包系统使用redis的队列保存所有小红包的金额，有一个请求过来，则从队列执行lpop出队一个金额。

因为redis指令的原子执行的，因此**lpop指令就保证了多个请求会同步序获取小红包金额**（其实也是利用了Redis命令的原子性），然后再执行hsetnx操作，插入抢红包详情。

这里redis队列的lpop指令其实也起到了一个分布式锁的作用。



<br/>



**你们系统使用了哪些设计模式**

- **模板方法模式**
  - 定时任务执行框架，把整体算法逻辑，也就是获取分布式锁，计算耗时、告警 这些通用操作放到父类，把真正执行业务代码的run()方法延迟到子类实现。
- **代理模式**
  - 静态代理：比如说调用外部接口，就写一个静态代理，进行参数校验，如果校验通过，才发起远程调用。
  - 动态代理：日志打印
- **组合模式**
  - 比如说CRM工单系统的左侧菜单，这个菜单使用组合模式，每一个元素既可以是一个菜单（菜单下面可以有菜单和菜单项），也可以是一个菜单项（叶子节点），这些元素都实现统一的接口。客户端无需判断是菜单，还是菜单项，统一处理。
  - 通过单一职责换取透明性。
- **责任链模式**
  - 比如说我曾经做过一个微信公众号对接的项目，微信公众号会推送各种不同类型的消息到咱们的商户后台，咱们后台使用责任链模式来处理，这些处理者有粉丝关注消息的处理者、点击公众号菜单的处理者，每一个处理者都会得到这个请求消息，请求消息中包含类型字段，当处理者判断消息的类型是由它处理的，则进行处理，否则，将消息传递给下一个处理者进行处理。
- **适配器模式**
  - 比如说CRM工单系统中，获取地图信息的接口A（新接口），但是客户端（比如说另外一个service）依赖的是接口B（老接口），接口A和接口B的具体实现逻辑实际上是一样的（只不过接口A进行了代码整理），这时候想让客户端使用接口A的功能，为了不改变客户端代码，我们使用了适配器模式，创建一个适配器，实现了接口B，适配器持有接口A实现类的实例，真正的方法调用由该实例来执行。



<br/>



**你们系统使用了哪些设计原则**

- **针对接口编程**
  - 为了便于维护，避免实现类换了，导致客户端代码跟着改动
- **把应用中可能变化的代码和不经常变化的代码分离开来**
  - 比如说解析excel的工具类，就是不经常改变的代码，因此要从业务代码中抽离出来
  - 为了解耦，便于维护。避免经常改变的代码导致不经常改变的代码也跟着改变。
- **多用组合，少用继承**
  - 组合方便解耦，接口内部改变了，客户端代码不用动。
  - 继承的话，父类增加、删除了一些东西，子类也要跟着改。
  - 组合也可以用来实现多态。
- **类应该对扩展开放，对修改关闭**
  - 如果要增加一个类的功能，可以使用装饰者模式
- **依赖倒置原则，不要依赖具体实现类**
  - 要依赖抽象，比如说接口
- **底层组件不要调用高层组件**
  - 避免引起代码混乱
- **单一职责**
  - 一个类应该只有一个职责，从而只有一个引起变化的原因。这样子设计的类是高内聚的



<br/>



**如果hsetnx实际上是成功的，但是redis返回超时，这种情况怎么处理?**

这种情况实际上导致了数据的不一致性，咱们系统目前做了如下处理：

- 目前没有处理这种情况。假如要处理的话，增加一种补偿机制：

- 小红包表的表结构增加小红包id字段，如果一个人抢到红包了，则在内存中就有一个hash结构的抢红包详情列表。整体的数据结构为：
  - luckyMoneyId  userId  json
  - 这个json字符串包含小红包id、金额
- 扫描数据库中过去1天内的大红包数据（1天是因为红包1天过期），用这些大红包的id去redis进行Redis中检索（Scan指令），把该红包数据读到内存中，判断该红包的hash结构内，哪些人抢了小红包，然后根据小红包id去小红包数据库表中查看该小红包有没有执行转账操作

- 如果该小红包金额的数据在数据库没有执行转账操作（抢人员为空，并且转账订单id为空），但是在redis中有人抢了（存在hash结构中），那么就把数据库中小红包的抢人员id更新为该用户id，并且执行转账操作，更新转账订单id。



<br/>



**如果抢红包的时候，缓存中的红包数据没有了，怎么办？**

能走到抢红包这一步，证明发红包成功的，证明数据库中有这个红包数据，这时候会通过查询数据库得到红包数据，重新存入到redis中。（缓存中没有会查数据库）



<br/>



**支付系统产品上面的思考？**

红包这个业务不仅是简单的发红包、拆红包。可以用于更加丰富的场景，比如说红包可以跟推广文章绑定在一起，如果用户向下拉，并且在当前页面阅读了一定的时间，则自动弹出一个红包。用来激励用户阅读该文章。

简单来说，就是希望通过红包来激励用户，从而促进业务的发展。



<br/>



**怎么处理红包缓存和数据库一致性的问题？**

发红包过程中，发生缓存和数据库一致性问题，这种情况极少，缓存更新失败 或者 数据库更新失败，都会有告警，人工进行数据修复。



抢红包的过程中，发生缓存和数据库一致性问题，分为2种情况：

- 缓存更新成功了，数据库更新失败了
  - 针对这种情况，我们做了一个定时任务，每10分钟跑一次。定时任务参考上面的问题：“如果hsetnx实际上是成功的，但是redis返回超时，这种情况怎么处理?”
- 缓存更新失败了，数据库更新成功了
  - 理论上不存在这种情况，因为我们代码里面是缓存更新返回成功了，才会去更新数据库。



<br/>





**怎么处理缓存故障？**

我们使用sential哨兵集群来保障缓存高可用。假如真的主节点挂了，这一次请求更新失败，后面的数据库操作也不会进行，因此不会有问题，后面主从切换成功以后，客户端向sential重新获取可用的主节点地址，整个业务即可正常运转。



<br/>



**如何保证红包缓存和数据库的一致性？**

我们的红包系统，只有在缓存更新成功的前提下，才会去更新数据库。所以只会出现缓存更新成功，数据库更新失败导致数据不一致的情况。

- （1）通过重试、补偿来解决数据不一致的情况。
  - a、重试指的是数据库操作重试5次，依然失败则发送告警。
  - b、补偿可以是定时任务，也可以是人工补偿。 

下面根据具体场景来说下：

- （1）如果是发红包出现这种情况，那么就是缓存中插入这条红包数据，而数据库没有插入。那么开发人员会收到告警，通过查看日志找到这条数据，手动进行插入。
- （2）如果是抢红包出现这种情况，就比较复杂了。有一个补偿的定时任务，我们通过红包id去缓存中查找到小红包列表，然后跟数据库中的小红包列表逐个对比，如果缓存中这个小红包被抢了并且数据库中该小红包还没有被抢， 则说明数据不一致，这时候数据库中该小红包补充下被抢信息。





<br/>



**支付系统的相关参数？**

- 峰值QPS：监控数据是800/s，压测数据是12000/s
- PV：一天的PV（访问次数）为32万
- DAU：日活跃用户数，一天10万左右
  - 一天的UV（访问用户数）也是10万左右
- RT：接口响应时间一般是10毫秒 到 几百毫秒
  - 读红包接口一般是 10毫秒
  - 抢红包接口（写）也是10毫秒左右，因为直接写完redis，发送mq就返回了
- 系统用户总量：300万
- **红包缓存命中率** 99.99，因为缓存里面存放最近3天的数据，而极少有人去查看3天以前的红包
- 红包缓存的数据量：600M（600兆）



<br/>



**支付系统的高性能、高可用、高扩展？**

- 高性能：缓存、异步、分表、分布式架构、集群
- 高可用：集群
- 高扩展：
  - 服务拆分，可以很容易进行水平扩展。在过年过节的时候，可以通过增加机器来增加系统的并发能力
  - 通过增加配置即可以接入新的支付业务



<br/>



**支付系统中遇到比较有技术挑战的问题？ 比较难的问题？调账的使用场景？**

并发更新资金账户导致丢失更新的问题。

前提：同一时刻只允许一个操作更新资金账户，资金账户使用乐观锁。

具体场景：甲在发一个B红包的时候，乙抢了A红包，这个A红包是甲之前发的红包，发红包会冻结资金，抢红包会解冻资金，这两个资金操作同时发生，同时修改去用户的资金账户，由于资金账户使用了版本号作为乐观锁，从而第二个资金操作记录失败了，因此乙抢了A红包就没有入账。

临时的解决方案：就是调账，把甲的钱包减去这笔钱，把用户乙的钱包加上这笔钱。

最终的解决方案：使用补偿机制，目前是10分钟一次，扫描之前转账失败的红包，重新触发转账。

<br/>



## 参考

[快钱企业人民币网关，对应我们的充值业务](https://wenku.baidu.com/view/06ff0e68998fcc22bdd10d5a.html)   

[支付系统演变过程](https://www.sohu.com/a/124684023_355140)

